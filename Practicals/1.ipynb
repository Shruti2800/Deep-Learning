{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3987200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow \n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//FQ4021TU//Desktop//Practicals//1_boston_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6bd6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox  ...  tax  ptratio       b  lstat  MEDV\n",
       "0  0.00632  18.0   2.31     0  0.538  ...  296     15.3  396.90   4.98  24.0\n",
       "1  0.02731   0.0   7.07     0  0.469  ...  242     17.8  396.90   9.14  21.6\n",
       "2  0.02729   0.0   7.07     0  0.469  ...  242     17.8  392.83   4.03  34.7\n",
       "3  0.03237   0.0   2.18     0  0.458  ...  222     18.7  394.63   2.94  33.4\n",
       "4  0.06905   0.0   2.18     0  0.458  ...  222     18.7  396.90   5.33  36.2\n",
       "5  0.02985   0.0   2.18     0  0.458  ...  222     18.7  394.12   5.21  28.7\n",
       "6  0.08829  12.5   7.87     0  0.524  ...  311     15.2  395.60  12.43  22.9\n",
       "7  0.14455  12.5   7.87     0  0.524  ...  311     15.2  396.90  19.15  27.1\n",
       "8  0.21124  12.5   7.87     0  0.524  ...  311     15.2  386.63  29.93  16.5\n",
       "9  0.17004  12.5   7.87     0  0.524  ...  311     15.2  386.71  17.10  18.9\n",
       "\n",
       "[10 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc5ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.loc[:,df.columns!='MEDV']\n",
    "y=df.loc[:,df.columns=='MEDV']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab02019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FQ4021TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mms = MinMaxScaler()\n",
    "mms.fit(X_train)\n",
    "X_train=mms.transform(X_train)\n",
    "X_test=mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68db616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,input_shape=(13,),activation='relu',name='dense_1'))\n",
    "model.add(Dense(64,activation='relu',name='dense_2'))\n",
    "model.add(Dense(1,activation='linear',name='dense_output'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "febe9d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\FQ4021TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 128)               1792      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10113 (39.50 KB)\n",
      "Trainable params: 10113 (39.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d64522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\FQ4021TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\FQ4021TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "11/11 [==============================] - 2s 21ms/step - loss: 585.3481 - mae: 22.3334 - val_loss: 602.0879 - val_mae: 22.5977\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 545.8417 - mae: 21.3705 - val_loss: 554.7259 - val_mae: 21.4746\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 488.6059 - mae: 19.8741 - val_loss: 477.3788 - val_mae: 19.5019\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 395.2896 - mae: 17.3053 - val_loss: 361.5376 - val_mae: 16.0645\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 277.4624 - mae: 13.5860 - val_loss: 230.7741 - val_mae: 11.7003\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 178.4084 - mae: 10.5275 - val_loss: 150.1128 - val_mae: 8.5852\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 142.2971 - mae: 9.3483 - val_loss: 129.5012 - val_mae: 8.2726\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 127.5431 - mae: 8.7611 - val_loss: 118.2890 - val_mae: 7.8466\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 109.2981 - mae: 7.9563 - val_loss: 112.1854 - val_mae: 7.4611\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 96.7415 - mae: 7.3016 - val_loss: 102.6534 - val_mae: 7.0497\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 84.6957 - mae: 6.7736 - val_loss: 92.3050 - val_mae: 6.6823\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 74.6055 - mae: 6.2983 - val_loss: 86.2131 - val_mae: 6.3763\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 66.8529 - mae: 5.8809 - val_loss: 81.1935 - val_mae: 6.2120\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.5357 - mae: 5.5418 - val_loss: 77.3054 - val_mae: 6.2004\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.3221 - mae: 5.3289 - val_loss: 74.2478 - val_mae: 6.1754\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.0533 - mae: 5.1366 - val_loss: 71.8730 - val_mae: 6.1043\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.8114 - mae: 4.9446 - val_loss: 70.5231 - val_mae: 5.9610\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 48.6812 - mae: 4.8342 - val_loss: 68.4146 - val_mae: 5.9666\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 46.7266 - mae: 4.7684 - val_loss: 66.6806 - val_mae: 5.9422\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 45.0190 - mae: 4.6767 - val_loss: 65.1110 - val_mae: 5.8600\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 43.2215 - mae: 4.5883 - val_loss: 63.4185 - val_mae: 5.7926\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.6292 - mae: 4.4385 - val_loss: 62.5632 - val_mae: 5.5924\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.7227 - mae: 4.3378 - val_loss: 60.2430 - val_mae: 5.5915\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 38.2018 - mae: 4.3250 - val_loss: 57.9816 - val_mae: 5.6101\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 36.3783 - mae: 4.1753 - val_loss: 57.6919 - val_mae: 5.3363\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 34.9976 - mae: 4.0561 - val_loss: 55.7034 - val_mae: 5.3148\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 33.5135 - mae: 4.0545 - val_loss: 53.8945 - val_mae: 5.2774\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 31.9468 - mae: 3.9606 - val_loss: 52.2823 - val_mae: 5.1980\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 30.6677 - mae: 3.8728 - val_loss: 51.6211 - val_mae: 5.0402\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 29.5298 - mae: 3.6827 - val_loss: 50.7593 - val_mae: 4.9244\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 28.4573 - mae: 3.8171 - val_loss: 46.9046 - val_mae: 5.0508\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 27.1407 - mae: 3.6021 - val_loss: 48.2932 - val_mae: 4.7979\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 25.7670 - mae: 3.4949 - val_loss: 45.6744 - val_mae: 4.8034\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 24.9247 - mae: 3.4783 - val_loss: 45.7144 - val_mae: 4.7468\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 23.9727 - mae: 3.3269 - val_loss: 44.8342 - val_mae: 4.7168\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 23.2052 - mae: 3.3281 - val_loss: 43.0182 - val_mae: 4.7004\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 22.4990 - mae: 3.2555 - val_loss: 42.9525 - val_mae: 4.6471\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 21.8569 - mae: 3.2059 - val_loss: 42.7293 - val_mae: 4.6061\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 21.4200 - mae: 3.1905 - val_loss: 40.8003 - val_mae: 4.5906\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 20.6244 - mae: 3.1187 - val_loss: 42.7565 - val_mae: 4.4994\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 20.2515 - mae: 3.0772 - val_loss: 39.4208 - val_mae: 4.4940\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.8198 - mae: 3.0751 - val_loss: 40.2054 - val_mae: 4.4290\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 19.5075 - mae: 3.0294 - val_loss: 39.9757 - val_mae: 4.3732\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 19.2165 - mae: 2.9628 - val_loss: 39.4102 - val_mae: 4.3374\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 19.0335 - mae: 3.0227 - val_loss: 37.7161 - val_mae: 4.3051\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 18.9010 - mae: 2.9296 - val_loss: 38.4773 - val_mae: 4.2344\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 18.4252 - mae: 2.9464 - val_loss: 37.2367 - val_mae: 4.2097\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 18.0475 - mae: 2.9142 - val_loss: 36.9651 - val_mae: 4.1739\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 17.8650 - mae: 2.8679 - val_loss: 36.9099 - val_mae: 4.1298\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.7193 - mae: 2.8820 - val_loss: 36.7868 - val_mae: 4.1057\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.5212 - mae: 2.8365 - val_loss: 36.0064 - val_mae: 4.0621\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 17.4428 - mae: 2.8850 - val_loss: 35.5303 - val_mae: 4.0302\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.5345 - mae: 2.8339 - val_loss: 35.9247 - val_mae: 4.0010\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 17.1010 - mae: 2.8451 - val_loss: 34.6917 - val_mae: 3.9531\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.9428 - mae: 2.8269 - val_loss: 35.1191 - val_mae: 3.9345\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.8368 - mae: 2.8003 - val_loss: 35.2031 - val_mae: 3.9153\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.0178 - mae: 2.8276 - val_loss: 34.3826 - val_mae: 3.8869\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.5965 - mae: 2.7644 - val_loss: 35.0831 - val_mae: 3.8712\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.5273 - mae: 2.7606 - val_loss: 34.3030 - val_mae: 3.8326\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.6327 - mae: 2.8112 - val_loss: 34.2787 - val_mae: 3.8004\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.4313 - mae: 2.7790 - val_loss: 32.6996 - val_mae: 3.7492\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.1529 - mae: 2.7506 - val_loss: 34.4604 - val_mae: 3.7649\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.2468 - mae: 2.7367 - val_loss: 32.5829 - val_mae: 3.7112\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.3439 - mae: 2.7874 - val_loss: 33.5404 - val_mae: 3.7032\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.9758 - mae: 2.7226 - val_loss: 31.8141 - val_mae: 3.6499\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.9227 - mae: 2.7638 - val_loss: 32.6544 - val_mae: 3.6404\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.8493 - mae: 2.6991 - val_loss: 31.5916 - val_mae: 3.6045\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 15.8698 - mae: 2.7359 - val_loss: 31.8738 - val_mae: 3.5846\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.7586 - mae: 2.7422 - val_loss: 31.4038 - val_mae: 3.5530\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.6271 - mae: 2.6697 - val_loss: 31.4055 - val_mae: 3.5268\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.4938 - mae: 2.7093 - val_loss: 30.9333 - val_mae: 3.4983\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.4527 - mae: 2.6669 - val_loss: 30.4963 - val_mae: 3.4639\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 15.3105 - mae: 2.6774 - val_loss: 30.2945 - val_mae: 3.4392\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 15.4758 - mae: 2.6409 - val_loss: 30.9402 - val_mae: 3.4608\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.2272 - mae: 2.6843 - val_loss: 28.9336 - val_mae: 3.3864\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 15.2313 - mae: 2.6479 - val_loss: 30.3136 - val_mae: 3.4047\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.4241 - mae: 2.7370 - val_loss: 29.7898 - val_mae: 3.3754\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 15.3568 - mae: 2.6475 - val_loss: 29.2086 - val_mae: 3.3446\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 15.1580 - mae: 2.6967 - val_loss: 29.8006 - val_mae: 3.3691\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.0320 - mae: 2.6106 - val_loss: 28.9752 - val_mae: 3.3216\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 15.0499 - mae: 2.6739 - val_loss: 28.8876 - val_mae: 3.3045\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 14.7077 - mae: 2.6045 - val_loss: 27.8354 - val_mae: 3.2433\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 14.6238 - mae: 2.6213 - val_loss: 28.9754 - val_mae: 3.2769\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.6001 - mae: 2.5733 - val_loss: 28.5768 - val_mae: 3.2422\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.4748 - mae: 2.5922 - val_loss: 27.9193 - val_mae: 3.2135\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.3548 - mae: 2.5821 - val_loss: 27.0769 - val_mae: 3.1596\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.4617 - mae: 2.6139 - val_loss: 28.9166 - val_mae: 3.2294\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.2035 - mae: 2.5613 - val_loss: 26.6412 - val_mae: 3.1164\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 14.2028 - mae: 2.5939 - val_loss: 28.0500 - val_mae: 3.1529\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.3056 - mae: 2.5287 - val_loss: 26.0574 - val_mae: 3.0660\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 14.1367 - mae: 2.5544 - val_loss: 26.9109 - val_mae: 3.0890\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.0658 - mae: 2.5462 - val_loss: 25.8265 - val_mae: 3.0430\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.0697 - mae: 2.5128 - val_loss: 25.9540 - val_mae: 3.0244\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 13.8695 - mae: 2.5514 - val_loss: 25.6923 - val_mae: 3.0076\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 13.8751 - mae: 2.4962 - val_loss: 25.9354 - val_mae: 3.0142\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 13.7310 - mae: 2.5316 - val_loss: 25.0955 - val_mae: 2.9693\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 13.8134 - mae: 2.4917 - val_loss: 25.1612 - val_mae: 2.9488\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 13.5983 - mae: 2.5214 - val_loss: 25.5921 - val_mae: 2.9618\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 13.5962 - mae: 2.4821 - val_loss: 25.4928 - val_mae: 2.9595\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 13.7138 - mae: 2.5493 - val_loss: 25.1591 - val_mae: 2.9166\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=100,validation_split=0.05,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1d56107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 8400519.0000 - mae: 2863.4395\n",
      "Mean Squared Error: 8400519.0\n",
      "Mean absolute error: 2863.439453125\n"
     ]
    }
   ],
   "source": [
    "mse_nn,mae_nn=model.evaluate(X_test,y_test)\n",
    "print(\"Mean Squared Error:\",mse_nn)\n",
    "print(\"Mean absolute error:\",mae_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab574a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
